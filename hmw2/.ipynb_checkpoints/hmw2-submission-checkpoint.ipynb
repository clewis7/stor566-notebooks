{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ed006-0cab-4d89-b7fb-b627a75abaac",
   "metadata": {},
   "source": [
    "Downloading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc91b919-b3dd-4b04-903c-903d9e6a9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_dir = '/home/clewis7/repos/stor566-notebooks/hmw2/hmw2-data/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as td\n",
    "import random, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "## USE THIS SNIPPET TO GET BINARY TRAIN/TEST DATA\n",
    "\n",
    "train_data = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "# Once you have downloaded the data by setting download=True, you can\n",
    "# change download=True to download=False\n",
    "test_data = datasets.MNIST(data_dir, train=False, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "subset_indices = ((train_data.targets == 0) + (train_data.targets == 1)).nonzero()\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, \n",
    "  shuffle=False,sampler=SubsetRandomSampler(subset_indices.view(-1)))\n",
    "\n",
    "\n",
    "subset_indices = ((test_data.targets == 0) + (test_data.targets == 1)).nonzero()\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size, \n",
    "  shuffle=False,sampler=SubsetRandomSampler(subset_indices.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e83e1-bc7d-4b30-a528-9be7aadcc7e6",
   "metadata": {},
   "source": [
    "### 1. (20 points) Implement **Logistic Regression** with Pytorch to do handwritten digit 0 vs. 1 classification. Pick an optimizer yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "338889aa-92a3-43f2-a067-b4381c5ed052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae46d1d5-4a3c-490f-b599-a1fe6a643756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "            \n",
    "     def forward(self, x):\n",
    "         outputs = torch.sigmoid(self.linear(x))\n",
    "         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24a4cfdc-6e97-41c1-a68e-08dbc44a539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "output_dim = 1\n",
    "model = LogisticRegression(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a4b31ee-0669-467b-8a13-27f5856fd8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72b03bf9-fc65-4d76-8342-6f3136d02932",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20ddab13-61d2-4ca2-8e36-9095f37a3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78735d4a-6661-44fb-a731-ea46627379fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 196608",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Convert the 28*28 image matrix into a 784-dim vector\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Convert labels from 0,1 to -1,1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 196608"
     ]
    }
   ],
   "source": [
    "# The number of epochs is at least 10, you can increase it to achieve better performance\n",
    "num_epochs = 10\n",
    "avg_losses = list()\n",
    "batch_losses = list()\n",
    "\n",
    "# Training the Model\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Convert the 28*28 image matrix into a 784-dim vector\n",
    "        images = images.view(-1, 28*28) \n",
    "        # Convert labels from 0,1 to -1,1\n",
    "        labels = 2*(labels.float()-0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## TODO \n",
    "        # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images[1])\n",
    "#         loss = criterion(torch.squeeze(outputs), labels) \n",
    "#         total_loss += loss\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "        # 2. Do optimization. Check torch.optim to see how to do optimization with pytorch\n",
    "        # optimizer.step()\n",
    "        # # 3. Save batch loss\n",
    "        # batch_loss = total_loss / images.shape[0]\n",
    "        # batch_losses.append(batch_loss)\n",
    "\n",
    "    ## Save average epoch loss\n",
    "    # avg_epoch_loss = (1 / batch_size) * np.sum(batch_losses)\n",
    "    # avg_losses.append(avg_epoch_loss)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f12b21-ca53-4137-a3cf-2e2e9255bffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6cf317-f187-4a95-a323-8a3680a5dcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbed5d-1ec6-4f99-8630-ca11dfdae05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b946d6e-6777-4545-a208-9d04e5059380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364d16d6-dca3-48eb-97e9-4c4a76f5ea22",
   "metadata": {},
   "source": [
    "### (a) (5 points) Report the hyper-parameters (number of epochs, learning rate, momentum etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335c2db-0030-40eb-9401-94dd4fb37d65",
   "metadata": {},
   "source": [
    "**Hyper-Parameters**\n",
    "\n",
    "Number of Epochs: 10\n",
    "\n",
    "Learning Rate: 0.01\n",
    "\n",
    "Momentum: 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91469008-1c0c-4d61-a740-8791de9d66db",
   "metadata": {},
   "source": [
    "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29df3f16-a91d-4dbd-80de-97bdc38575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ee6b1-a314-4a9a-a8b1-c74a3b145dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d5e28-0f86-40f4-9122-663cd7e48c0b",
   "metadata": {},
   "source": [
    "### (c) (5 points) Report the final testing accuracy of trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e44882e9-0fe0-4cf8-b1a8-49f1b3914045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/clewis7/repos/stor566-notebooks/hmw2/hmw2-data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f0a0329-19db-46d2-bd97-3c9f82bda71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/clewis7/repos/stor566-notebooks/hmw2/hmw2-data/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48345321-d287-43be-95aa-40c23bbe52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted based on newly trained model\n",
    "for (images, labels) in test_loader:\n",
    "    predicted = model(\n",
    "actual_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5913bf-5f18-4aeb-b835-33d2202e43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "accuracy_sum = 0\n",
    "for i in range(N):\n",
    "    accuracy_sum += 1("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61859fa3-951f-4e03-aa34-fd5e6da40c83",
   "metadata": {},
   "source": [
    "## Problem 2 (60 points)\n",
    "\n",
    "In this problem you will practice implementing MLP and CNN to classify daily life images (CIFAR10).\n",
    "\n",
    "**Data.** You will use CIFAR10 classification dataset (10 classes). Pytorch/torchvision has provide a useful dataloader to automatically download and load the data into batches. Code of the data loader has been provided in the template. Please don't modify the data loading part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2ade048-17f3-4a89-bc11-398c89095e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loading code chunk, please don't modify it. \n",
    "## However, you can adjust the batch size if you want to.\n",
    "batch_size_cifar = 64\n",
    "data_dir = './data'\n",
    "\n",
    "def cifar_loaders(batch_size, shuffle_test=False): \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.225, 0.225, 0.225])\n",
    "    train = datasets.CIFAR10(data_dir, train=True, download=False, \n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    # Once you have downloaded the data by setting download=True, you can\n",
    "    # change download=True to download=False\n",
    "    test = datasets.CIFAR10(data_dir, train=False, \n",
    "        transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "        shuffle=True, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
    "        shuffle=shuffle_test, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = cifar_loaders(batch_size_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c841ac8-9aa0-46da-9cfa-2ff62eb110eb",
   "metadata": {},
   "source": [
    "### **Problem Description.**\n",
    "### 1. (20 points) Implement a 7 layers fully-connected neural networks with ReLU activation to do image classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac4041-3dc5-44c1-87cc-356074245e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef0faab1-37fa-40a9-9608-4f66b12b2a63",
   "metadata": {},
   "source": [
    "### (a) (5 points) Print the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718e720-b443-4506-acde-637297af7d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca7de90-0fc0-4033-ab32-2c5cf39e8689",
   "metadata": {},
   "source": [
    "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deead5f-43dc-425c-a33d-5e30eb03806d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38e1f77-8928-4499-a941-e9fc50e813ac",
   "metadata": {},
   "source": [
    "### (c) (5 points) Report the final testing accuracy of trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01fc7c-a81d-442b-b52b-f49914cec17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d6c45c7-c467-4c28-8b5c-e083b4472290",
   "metadata": {},
   "source": [
    "### 2. (30 points) Implement a 7 layers CNN with 4 convolutional layers, 3 fully-connected layers and ReLU activation function. The input dimension of the 1st fully-connected layer must be 4096."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a925fe-a829-4a0f-a9b2-cf71003bd1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "850fccfd-ffae-462a-ae45-57be7d99a19d",
   "metadata": {},
   "source": [
    "### (a) (5 points) Print the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6d065-f228-4bd0-87f1-5d54a35681d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fac679-ba16-4a95-b561-ce9983b0c6ca",
   "metadata": {},
   "source": [
    "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f65cae-2414-4e34-a88c-f7c284f1ccd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ca90f9-c7f0-436f-aa70-5c10f17d6ff6",
   "metadata": {},
   "source": [
    "### (c) (5 points) Report the final testing accuracy of trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9626f-3730-471c-934b-b9067e83afca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac3af683-0cbf-4d4c-842c-75d6fb0774bd",
   "metadata": {},
   "source": [
    "### (d) (10 points) Write a new cifar$\\_$loaders function to try different data augmentation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb420fe-fc0b-40d3-8c20-2ddbaa2ef61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf81f548-5e53-42e9-9c8e-9ccfabb57da5",
   "metadata": {},
   "source": [
    "### 3. (10 points) Please compare the results of the two models (MLP and CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39524d8a-881b-4cd1-b157-149ebb9d7158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
